{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING RNN FROM SCRATCH IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation¶\n",
    "We will implement a full Recurrent Neural Network from scratch using Python. We will try to build a text generation model using an RNN. We train our model to predict the probability of a character given the preceding characters. It’s a generative model. Given an existing sequence of characters we sample a next character from the predicted probabilities, and repeat the process until we have a full sentence. This implementation is from Andrej Karparthy great post building a character level RNN. Here we will discuss the implementation details step by step.\n",
    "\n",
    "General steps to follow:\n",
    "\n",
    "Initialize weight matrices U, V, W from random distribution and bias b, c with zeros\n",
    "Forward propagation to compute predictions\n",
    "Compute the loss\n",
    "Back-propagation to compute gradients\n",
    "Update weights based on gradients\n",
    "Repeat steps 2–5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize:\n",
    "\n",
    "To start with the implementation of the basic RNN cell, we first define the dimensions of the various parameters U,V,W,b,c.\n",
    "\n",
    "Dimensions: Let’s assume we pick a vocabulary size vocab_size= 8000 and a hidden layer size hidden_size=100. Then we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,hidden_size,vocab_size,seq_length,learning_rate):\n",
    "        # hyper params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # model params\n",
    "        \n",
    "        self.U= np.random.uniform(-np.sqrt(1./vocab_size),np.sqrt(1./vocab_size),(hidden_size,vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size),np.sqrt(1./hidden_size),(vocab_size,hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size),np.sqrt(1./hidden_size),(hidden_size,hidden_size))\n",
    "        self.b = np.zeros((hidden_size,1)) # bias for hidden layer.\n",
    "        self.c = np.zeros((vocab_size,1)) # bias for output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper initialization of weights seems to have an impact on training results there has been lot of research in this area. It turns out that the best initialization depends on the activation function (tanh in our case) and one recommended approach is to initialize the weights randomly in the interval from[ -1/sqrt(n), 1/sqrt(n)]where n is the number of incoming connections from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straightforward as per our equations for each timestamp t, we calculate hidden state hs[t] and output os[t] applying softmax to get the probability for the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,inputs,hprev):\n",
    "    xs,hs,os,ycap = {},{},{},{}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = zero_init(self.vocab_size,1)\n",
    "        xs[t][inputs[t]] = 1 # one hot encoding\n",
    "        hs[t] = np.tanh(np.dot(self.U,xs[t])+np.dot(self.W,hs[t-1])+ self.b) # hidden state\n",
    "        os[t] = np.dot(self.V,hs[t]) + self.c\n",
    "        ycap[t] = self.softmax(os[t])\n",
    "    return xs,hs,ycap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps/ np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/qp21xvjj4tq4tgn3cw0gltn40000gn/T/ipykernel_22712/3264919273.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  exps = np.exp(x)\n",
      "/var/folders/9c/qp21xvjj4tq4tgn3cw0gltn40000gn/T/ipykernel_22712/3264919273.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return exps/ np.sum(exps)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1000,2000,3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust the softmax function to handle larger numbers like this:\n",
    "\n",
    "def softmax(self,x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p / np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p / np.sum(p)\n",
    "\n",
    "# Example usage\n",
    "print(softmax([1, 2, 3]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "class MyClass:\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x - np.max(x))\n",
    "        return p / np.sum(p)\n",
    "\n",
    "# Create an instance of the class\n",
    "my_instance = MyClass()\n",
    "\n",
    "# Example usage\n",
    "print(my_instance.softmax([1, 2, 3]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self,ps,targets):\n",
    "    '''loss for a sequence'''\n",
    "    # cross entropy loss\n",
    "    return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Backward pass¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Repeat steps 2–5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our model to learn from the data and generate text, we need to train it for sometime and check loss after each iteration. If the loss is reducing over a period of time that means our model is learning what is expected of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train for some time and if all goes well, we should have our model ready to predict some text. Let us see how it works for us.\n",
    "\n",
    "We will implement a predict method to predict few words like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(rows, cols):\n",
    "    return np.zeros((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = zero_init(self.vocab_size,1)\n",
    "                xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "                ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = zero_init(self.vocab_size, 1)\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = zero_init(self.vocab_size,1)\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            threshold = 0.01\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "            while (smooth_loss > threshold):\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geit ilmfsefsbeei:edsnessbhss.umohv:aitmsnsw.Ive:aatpeis aspogaay.fhhgsss,Iivmsfslassssfylhdg.,Hseasb:stessisiae:msHpssmlwt,I,aaHbeysoa asbop:yunisiissvhlsse psgaagiseosmepssisvusbpitIwssisba,.w.ssHus\n",
      "\n",
      "\n",
      "iter :0, loss:80.472036\n",
      " day, the sun is s beaulip l day, the sun iu shinin ss a beautiful day, the sun is shining and I love tl do whattpas I beautiful day, the sun isishiningnand and do whattin as shiningnan is shining and\n",
      "\n",
      "\n",
      "iter :500, loss:58.984736\n",
      "ing I love to do whathin is shining and I love to do whatt aya i  do whatisul day, the sun is shining ao bha t is shining and I love to do whatt whattin: s andnI lovedto do whato whatt e ss  if shinin\n",
      "\n",
      "\n",
      "iter :1000, loss:35.917290\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatting s d le utifulivn is shining and I love to do whato bpis a beautiful day, the sun is shining and\n",
      "\n",
      "\n",
      "iter :1500, loss:21.843763\n",
      "o whatiin  s dove to do whatting s dove to do whattin is shining and I love to do whattis is shining and I love to do whatiful day, the sun is shining and I love to do whatove sun is shining and I lov\n",
      "\n",
      "\n",
      "iter :2000, loss:13.286743\n",
      "ing and I love to do whato whatiful day, the sun is shining and I love to do whatifnl day, the sun is shining and I love to do whatt esss ii is a beautiful day, the sun is shining and I love to do wha\n",
      "\n",
      "\n",
      "iter :2500, loss:8.086961\n",
      "iful day, the sun is shining and I love to do whato whattind I love to do whatt essshinb and I love to do whato e to any,d to whato wua is a beautiful day, the sun is shining and I love to do whattpnd\n",
      "\n",
      "\n",
      "iter :3000, loss:4.927512\n",
      "o whatiful day, the sun is shining and I love to do what, whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whattin is shining and I love to do whatif\n",
      "\n",
      "\n",
      "iter :3500, loss:3.007491\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whatt is shining and I love to do whato whatting and I love to do whatiful day, the sun is shining and I love to do whatiful doy,\n",
      "\n",
      "\n",
      "iter :4000, loss:1.840253\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato is shining and I love to do whatiful day, the sun is shining and I love to do whatt is shining and\n",
      "\n",
      "\n",
      "iter :4500, loss:1.130268\n",
      "o whatiful day, thelsun is shining and I love to do whathind I love to do whatiful day, the sun is shining and I love to do whato whato whatiful day, the sun is shining and I love to do whatidisus in \n",
      "\n",
      "\n",
      "iter :5000, loss:0.698053\n",
      " is ahbnasu duy dnd Iay,ve to d I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato n as shining and I love to do whatiful day, the s\n",
      "\n",
      "\n",
      "iter :5500, loss:0.434602\n",
      "iful day, the sun is shining and I love to do whatifil day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatt is shining and I love to do whato ay, the sun is\n",
      "\n",
      "\n",
      "iter :6000, loss:0.273754\n",
      "o whatt es s beautiful day, the sun is shining and I love to do whatt is shining and I love to do whatiful day, the sun is shining and I love to do whatt is shining and I love to do whathfnd Ial ve tt\n",
      "\n",
      "\n",
      "iter :6500, loss:0.175312\n",
      "ing and I love to do whato e ss a beautiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatt whatiful \n",
      "\n",
      "\n",
      "iter :7000, loss:0.114847\n",
      "iful day, the sun is shining and I love to do whato isushining and I love to do whatifbl day, the sun is shining and I love to do whato whathing and I love to do whato whatiful day, the sun is shining\n",
      "\n",
      "\n",
      "iter :7500, loss:0.077533\n",
      "o whatiful day, the sun is shining and I love to do whato whatt as shining and I love to do whatifuliday, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato wh\n",
      "\n",
      "\n",
      "iter :8000, loss:0.054347\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatting and I love to do wha\n",
      "\n",
      "\n",
      "iter :8500, loss:0.039792\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do what\n",
      "\n",
      "\n",
      "iter :9000, loss:0.030535\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato whatiful day, the sun is shining and I love to d\n",
      "\n",
      "\n",
      "iter :9500, loss:0.024540\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whato whatiful day, the sun is shining and I love to do whato whato is shining and I love to do whattfuy, the sun is shining and \n",
      "\n",
      "\n",
      "iter :10000, loss:0.020558\n",
      "iful day, the sun is shining and Iolove to do whatt as d be ut ds whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whattfuy, th ds: it is shining and\n",
      "\n",
      "\n",
      "iter :10500, loss:0.017837\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatn nI I bevuttfuliful day, the sun is shining and I love to do whatiful day, the sun is shining and I\n",
      "\n",
      "\n",
      "iter :11000, loss:0.015913\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whatoinsis shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love\n",
      "\n",
      "\n",
      "iter :11500, loss:0.014492\n",
      "iful day, the sun is shining and I love to do whattiu is a beautiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato whatt is shining and I love to do \n",
      "\n",
      "\n",
      "iter :12000, loss:0.013406\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do what\n",
      "\n",
      "\n",
      "iter :12500, loss:0.012543\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whathin is shining and I love to do whatn ng s d beautnfulivn is shining and I love to do whattind I love to do whathfnl I, isa s\n",
      "\n",
      "\n",
      "iter :13000, loss:0.011828\n",
      "iful day, the sun is shining and I love to do whathing a l ve to do whatiful day,pthe sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining\n",
      "\n",
      "\n",
      "iter :13500, loss:0.011222\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whathing and I love to do whattfulw a: it is a ining a\n",
      "\n",
      "\n",
      "iter :14000, loss:0.010697\n",
      "ing and I love to do whathin is shining and I love to do what, whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and \n",
      "\n",
      "\n",
      "iter :14500, loss:0.010229\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"/Users/jasper/Desktop/RNN_from_scratch/input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader2:\n",
    "    def __init__(self, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        #self.fp = open(path, \"r\")\n",
    "        #self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imtulllaorxthaolallylcddorlrdxxmlyrllllrrlrdrl lxrlrlllldprrltlfrllxol rltfnronaldrrlfduoxtah trylodg nllullt lddlflrryelrblrlgoyrdldhgarrllotailslrc rllpxtsflyyfltylllllcmlxhdarodr.he.nxlllxlllllxrll\n",
      "\n",
      "\n",
      "iter :0, loss:77.276622\n",
      "thstest this. maybe nou long text to test this. maybe not perfect but should get you goipe test this. maybe not perfect but should get you peald get you goirxt should get you goilully lot t but should\n",
      "\n",
      "\n",
      "iter :500, loss:53.682609\n",
      "ct but should get you goieet but should get bou notext tut this. maybe not perfect but should get you goiegt but should get you goitethis. maybe not perfect but should get you goise iest this.fmaybt t\n",
      "\n",
      "\n",
      "iter :1000, loss:32.731891\n",
      "est this. maybt te seut shoully lo get you goib thyut should get you goieft but should get you goilhnbe not perfect but should get you goi trse thbut should get you goild get youlg text to test this. \n",
      "\n",
      "\n",
      "iter :1500, loss:19.935701\n",
      "ect but should get you goibet but should get you goiset buthshould get you goie t but should get you goiext but should get you goit thould get you goipet yst phould get you goieet but should get you g\n",
      "\n",
      "\n",
      "iter :2000, loss:12.146276\n",
      "ct bt soulg text to test this. maybe not perfect but should get you goieut yhis. maybe not perfect but should get you goie tibut should gyt you goiedt but should get you goi toie tbbu oou so pgetext t\n",
      "\n",
      "\n",
      "iter :2500, loss:7.407852\n",
      "est this. malx get you goieft tot this. maybe not perfect but should get you goild get you goitgtt to t this. maybe not perfect but should get you goitat this. maybe not perfect but should get you goi\n",
      "\n",
      "\n",
      "iter :3000, loss:4.525609\n",
      "ld get you goishoie tost tet you geit toyte thrsealdy lyng text to test this. maybe not perfect but should get you goilu get you goilt get you goit toot xre text to test this. maybe not perfect but sh\n",
      "\n",
      "\n",
      "iter :3500, loss:2.771958\n",
      "ct but should get you goiegt yot should get you goix t but should get you goit noe perfect but nhould get you goie tobt teut this. mayot to test this. maybt not perfect but should get you goit toy  oo\n",
      "\n",
      "\n",
      "iter :4000, loss:1.704347\n",
      "es hould get you goieet but should get you goied tett but should get you goilat yheully long text to test this. maybe not perfect but should get you goishould get you goipet but should get you goib to\n",
      "\n",
      "\n",
      "iter :4500, loss:1.053833\n",
      "t thbuld get you goipet but should get you goithnot thosld get you goild get you goibet but should get you goit thould get you goiect but should get you goitet but phould get you goibet but should get\n",
      "\n",
      "\n",
      "iter :5000, loss:0.656963\n",
      "ct but should get you goiset bbt should get you goiedt but should get you goiegt but should get you goiet text to test this. maybe not perfect but should get you goiedt but should get you goild get yo\n",
      "\n",
      "\n",
      "iter :5500, loss:0.414380\n",
      "est yhis. maybe bot perfect but should get you goisfoie not perfect but should get you goi.hoieg tett but should get you goishou goit toe  ooue not perfect but should get you goiset this. maybe not pe\n",
      "\n",
      "\n",
      "iter :6000, loss:0.265725\n",
      "t tobut tulynyb to test this. maybe not perfect but should get you goilhnle long text to test this. maybe not perfect but should get you goibet but should get you goishoied xbt tout shouldmget be t th\n",
      "\n",
      "\n",
      "iter :6500, loss:0.174298\n",
      "ct but should get you goipe iout shis.goiegt this. maybe not perfect but should get you goiext but should get you goisd le tot bot perfect but should get you goitet but should get you goild get you go\n",
      "\n",
      "\n",
      "iter :7000, loss:0.117767\n",
      "out shouldygett to test this. mayot to test this. maybe not perfect but should get you goiedt but shoald get you goiett bot should get you goilct to test this. maybe not perfect but should get you goi\n",
      "\n",
      "\n",
      "iter :7500, loss:0.082567\n",
      "t tybut should get you goipet yut should get you goiegt yot should get you goiyhould get you goiebt bot should get you goiegt bot should get you goiext bot should get you goie tobt you goiset this. ma\n",
      "\n",
      "\n",
      "iter :8000, loss:0.060430\n",
      "ct but should get you goishoie not perfect to test this. maybt not perfect but should get you goild get you goit tbet you goitet yut should get you goiest but should get you goild get you goieet but s\n",
      "\n",
      "\n",
      "iter :8500, loss:0.046306\n",
      "esl t lo get xt to test this. maybe not perfect but should get you goilhnget you goit thbut ihould get you goit toyt  to  eot pelu gotext to test this. maybe not perfect but should get you goila get y\n",
      "\n",
      "\n",
      "iter :9000, loss:0.037132\n",
      ".t test to test this. mayut thbut should get you goisgoiett st you goit t bu goi toiext this. maybe not perfect but should get you goi goie tot bot perfect but should get you goishoung xext to test th\n",
      "\n",
      "\n",
      "iter :9500, loss:0.031031\n",
      "ct but should get you goit tont  ou goishould get you goiet iext this. maybe not perfect but should get you goishoulg yeut shis. maybe not perfect but should get you goi hoisd ge t this. maybe norfeon\n",
      "\n",
      "\n",
      "iter :10000, loss:0.026842\n",
      "est this. maybt to test thiu. mayully long text to test this. maybe not perfect but should get you goiett to test this. mayot this. maybe not perfect but should get you goisd get you goit tobut iut sh\n",
      "\n",
      "\n",
      "iter :10500, loss:0.023869\n",
      "t tobt you goit this. malbe not perfect but should get you goitetisut should get you goild get you goild get you goisxt this. maybe not perfect but should get you goisfoie tot bot should get you goild\n",
      "\n",
      "\n",
      "iter :11000, loss:0.021679\n",
      "ct but should get you goishone not to test this. maybt this. maybe not perfect but should get you goie tibt sou goiett but should get you goishou goit tot perfect but should get you goild get you gois\n",
      "\n",
      "\n",
      "iter :11500, loss:0.019992\n",
      "est this. maybt to test this. mayot thiut should get you goisaoie not perfect but should get you goilx  eot t long text to test this. maybe not perfect but should get you goitetisut should get you goi\n",
      "\n",
      "\n",
      "iter :12000, loss:0.018649\n",
      "ect but should get you goiyhould get you goied tett tu test this. malo got test this. maybe not perfect but should get you goiett to test this. mayo  euld get you goishoue noe nonget oo perfest thbut \n",
      "\n",
      "\n",
      "iter :12500, loss:0.017544\n",
      "ct but should get you goile get you goiset this. maybe not perfect but should get you goit not perfect tut this. maybe not perfect but should get you goishoulg test to test thbut shbally long text to \n",
      "\n",
      "\n",
      "iter :13000, loss:0.016601\n",
      "est this. maybt soe perfect but should get you goibet but should get you goishoi. not perfect but should get you goist test this. maybe not perfect but should get you goibet but should get you goi foo\n",
      "\n",
      "\n",
      "iter :13500, loss:0.015783\n",
      ". tobt you goiehoie not perfect but should get you goiett to test this. malo  oimt iet gou goiett to test this. maybe not perfect but should get you goild get you goild get you goit this. maybe not pe\n",
      "\n",
      "\n",
      "iter :14000, loss:0.015061\n",
      "ct but should get you goit not perfect but should get you goithobe to test toise thould get you goit thould get you goiset this. maybe not perfect but should get you goiect but should get you goi toit\n",
      "\n",
      "\n",
      "iter :14500, loss:0.014409\n",
      "est this. mayot te test this. maybe not perfect but should get you goilt get you goie toyt you goishoie not tetteot this. maybe not perfect but should get you goild get you goisaobe not perfect but sh\n",
      "\n",
      "\n",
      "iter :15000, loss:0.013818\n",
      "tetybut should get you goispout toist thoilonget xt to test this. maybe not perfect but should get you goisxoget you goishoie not perfect but should get you goishould nou gou goi toie ooe long text to\n",
      "\n",
      "\n",
      "iter :15500, loss:0.013280\n",
      "ct but should get you goiset bhould get you goieet but should get you goishoit not perfect but should get you goilxt but should get you goisgt to test thlu goild get you goild get you goilanget you go\n",
      "\n",
      "\n",
      "iter :16000, loss:0.012781\n",
      "eut thbullyhbully long text to test this. maybe not perfect but should get you goishoied iou goishoie not perfect but should get you goishou. noe te test this. mayot thiut should get you goild get you\n",
      "\n",
      "\n",
      "iter :16500, loss:0.012320\n",
      "t tobt you goiehou goiet you goiext but should get you goishould you goied test this. maybe not perfect but should get you goieeniet you goisgoie toe long text to test this. maybe not perfect but shou\n",
      "\n",
      "\n",
      "iter :17000, loss:0.011893\n",
      "ct but should get you goi hois mxt  o g text to test this. maybe not perfect but should get you goiegt bot soouperfect but should get you goiect but should get you goisaobe not perfect but should get \n",
      "\n",
      "\n",
      "iter :17500, loss:0.011492\n",
      "est this. mayut thbu  yhbuldyg t you goiect but should get you goie thyut should get you goiset bhould get you goilhnbe long text to test this. maybe not perfect but should get you goieet but should g\n",
      "\n",
      "\n",
      "iter :18000, loss:0.011117\n",
      "t this. moyut not perfect but should get you goisd get you goishould nou goutgt not perfect but should get you goiet test this. maybe not perfect but should get you goit toit  oo goiset soo perfect bu\n",
      "\n",
      "\n",
      "iter :18500, loss:0.010768\n",
      "ct but should get you goishnut mong  not perfect but should get you goib toyt you goi text to test this. maybe not perfect but should get you goishoie  oo long text to test this. maybe not perfect but\n",
      "\n",
      "\n",
      "iter :19000, loss:0.010437\n",
      "out thbut thbut should get you goisct but should get you goiedt bot should get you goi goie not perfect but should get you goieet but should get you goilx get you goile le lon perfext shis. pet loog t\n",
      "\n",
      "\n",
      "iter :19500, loss:0.010125\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader2(seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hellogeHiuo: dH:muHgI:gIua gdya:hm,mgsuuy abuhf:noimifd\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Define a function to initialize a zero vector\n",
    "def zero_init(size, dim):\n",
    "    return np.zeros((size, dim))\n",
    "\n",
    "# Define the RNN class (as provided in your code)\n",
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = zero_init(self.vocab_size,1)\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            #through softmax\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "            #calculate dV, dc\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dc\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) \n",
    "        return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        sample a sequence of integers from the model\n",
    "        h is memory state, seed_ix is seed letter from the first time step\n",
    "        \"\"\"\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "        iter_num = 0\n",
    "        threshold = 0.01\n",
    "        smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "        while (smooth_loss > threshold):\n",
    "            if data_reader.just_started():\n",
    "                hprev = np.zeros((self.hidden_size,1))\n",
    "            inputs, targets = data_reader.next_batch()\n",
    "            xs, hs, ps = self.forward(inputs, hprev)\n",
    "            dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "            loss = self.loss(ps, targets)\n",
    "            self.update_model(dU, dW, dV, db, dc)\n",
    "            smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "            hprev = hs[self.seq_length-1]\n",
    "            if not iter_num%500:\n",
    "                sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "            iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt\n",
    "\n",
    "# To read the training data and make a vocabulary and dictionary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()\n",
    "\n",
    "# Example usage\n",
    "# Create a DataReader instance\n",
    "data_reader = DataReader('/Users/jasper/Desktop/RNN_from_scratch/input.txt', seq_length=25)\n",
    "\n",
    "# Create an RNN instance\n",
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 0.1\n",
    "rnn = RNN(hidden_size, data_reader.vocab_size, seq_length, learning_rate)\n",
    "\n",
    "# Predict the next characters starting with a given word\n",
    "start_word = \"hello\"\n",
    "num_chars_to_predict = 50\n",
    "predicted_text = rnn.predict(data_reader, start_word, num_chars_to_predict)\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eutnaytalb,tpinbd,y s aeHum,anfdmuivets isnis:is is:is:isuewHisdis:id:i,:bgnioudsnisfiv is is is is  steHuvs:Hwtes islis is isgIme  ismwIIis is is is isais is ispildoslisnisaisuibbis is:is asg,, is is\n",
      "\n",
      "\n",
      "iter :0, loss:80.472025\n",
      "bh,degnI laanhaau i:es tebsantevnd s , I bofehsahe o   aig ahhss gss s aty dggss i anhiiagoha.iIawdsig Ip lsul o  sfhde ulowahss ifouytI a apg s diiug in ts h a g a yoduloI g  iisiul  uyoteht ,lti m :\n",
      "\n",
      "\n",
      "iter :500, loss:74.927583\n",
      "anan itis is: iteis a beautiodloveyt ahe nda fhI tha whatwhdfthaabha whattha whav ti: d wnldve to do whatitioalo whatwheabdo whasado what lo whatwhadahofwhe snn sg a i shit ns a d I be thatHypoehatwho\n",
      "\n",
      "\n",
      "iter :1000, loss:53.647586\n",
      "s is s is:nio is and I love t  s a beauit:  ishin: it is a beautifuu day, the iuning and I love to do whatwhatihatwhatil,I love to do whd is a beautiful day, the sun is shining and I love to do whatwh\n",
      "\n",
      "\n",
      "iter :1500, loss:33.713689\n",
      "syo thatbha whavitt the sun is shining snd I love to do whataho thatitg whatido whatay, tha sun is a beautiful day, th  sun is a beautifuloday, t t is shining and I love to do whatius is a beautiful d\n",
      "\n",
      "\n",
      "iter :2000, loss:21.008853\n",
      "ing and I love to s day, the  tiful day, the sun is shining and I love to do whatiust in issshining and I loveutfna I love to do whatiuss is: it is a beautiful day, the sun is n ining a is: it is a be\n",
      "\n",
      "\n",
      "iter :2500, loss:13.061244\n",
      " ful day, the sun is shbeautioul day, the sun is shining and I love to do whatiuss it: it is a beautiful day, the sun is a beautiful day, the sun is shining and I love to do whatwhatimowabotwhatahe su\n",
      "\n",
      "\n",
      "iter :3000, loss:8.072134\n",
      "whatis shining and I y, the sun is shining and I love to do whavis fnining and I love to do whatiuishining and I love to do whatis s ining and I love to do whatiuIsiising and I love to do whatipofsy a\n",
      "\n",
      "\n",
      "iter :3500, loss:5.004909\n",
      "ing and I Iove to da whatius: it is a beautiful day, the sun is shining and I love to do whatiuss ining and I love to do whataletwhatiua is a beautiful day, the sun is shining and I love to do whatis \n",
      "\n",
      "\n",
      "iter :4000, loss:3.121477\n",
      "iful day, the sun is shining and I love to do whatiun:hit is a beautiful day, the sun is shining and I love to do whatiu s g a beautiful day, the sun is shining and I love to do whatis a beautiful day\n",
      "\n",
      "\n",
      "iter :4500, loss:1.964151\n",
      "iuos is: it is a beautiful day, the sun is shining and I love to do whatitoful day, the sun is shining and I love to do whatis shining and I love to do whatishand I love to do whatiu shining and I lov\n",
      "\n",
      "\n",
      "iter :5000, loss:1.251456\n",
      "ing and I love to do whatwhatitifhboveyto do whatwhatihdtitdfwhatiy,fhb and I love to do whatiy,ful day, the sun is shining and I love to do whatiuaa beautiful day, the sun is shining and I love to do\n",
      "\n",
      "\n",
      "iter :5500, loss:0.811155\n",
      "iful day, the sun is shining and I love to do wheasun is shining and I love to do whatiuufsbeautiful day, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to d\n",
      "\n",
      "\n",
      "iter :6000, loss:0.537886\n",
      "ay, the sun is shininin is shining and I love to do whatah, iua is a beautiful day, the sun is shining and I love to do whatiubalbeautiful day, the sun is shining and I love to do whatitiful day, the \n",
      "\n",
      "\n",
      "iter :6500, loss:0.367211\n",
      "ing and I love to do whatit,ful day, the sun is shining and I love to do whatis:a beautiful day, the sun is shining and I do whatiuatileautiful day, the sun is shining and I love to do whatiu a beauti\n",
      "\n",
      "\n",
      "iter :7000, loss:0.259716\n",
      "iful day, the sun is shining and I love to do whatiunshining and I love to do whatiutfhbeautiful day, the sun is shining and I love to do whatiu shining and I love to do whatiu shining and I love to d\n",
      "\n",
      "\n",
      "iter :7500, loss:0.191269\n",
      "sy, the sun is shining and I love to do whatiyoful day, the sun is shining and I love to do whatis:iabeautiful day, the sun is shining and I love to do whatiu shining and I love to do whatay, the sun \n",
      "\n",
      "\n",
      "iter :8000, loss:0.146990\n",
      "ing and I love to do whatwyatiuifaioing and I love to do whatiutveiso do whatil day, the sun is shining and I love to do whatilovey, the sun is shining and I love to do whatwtatuyofwlatitaful aag ang \n",
      "\n",
      "\n",
      "iter :8500, loss:0.117776\n",
      "iful day, the sun is shining and I love to do whatis shining and I love to do whatiu.shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatil day, the sun is shining\n",
      "\n",
      "\n",
      "iter :9000, loss:0.098065\n",
      "isoahbeautiful day, the sun is shining and I love to do whatis shining and I love to do whatis shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatis shining and I\n",
      "\n",
      "\n",
      "iter :9500, loss:0.084395\n",
      "ing and I love to do whatis shining and I love to do whatis shining and I love to do whatiuo wh tilofwhatiloIay, the shn is shining and I love to do whatitifabeautifhi d beautiful day, the sun is shin\n",
      "\n",
      "\n",
      "iter :10000, loss:0.074584\n",
      "iful day, theisun is shining and I love to do whatis shining and I love to do whatit:shd I love to do whatis shining and I love to do whatiuaa beautiful day, the sun is shining and I love to do whatiu\n",
      "\n",
      "\n",
      "iter :10500, loss:0.067288\n",
      "ittfsbeautiful day, the sun is shining and I love to do whatin falove to do whatitiful day, the sun is shining and I love to do whatis shining and I love to do whatiue suning and I love to do whatiu s\n",
      "\n",
      "\n",
      "iter :11000, loss:0.061668\n",
      "ing and I love to do whatisha beautiful day, the sun is shining and I love to do whatiy,ful day, the sun is shining and I love to do whatiun is a beautiful day, the sun is shining and I love to do wha\n",
      "\n",
      "\n",
      "iter :11500, loss:0.057171\n",
      "iful day, the sun is shining and I love to do whatiug is a beautiful day, the sun is shining and I love to do whatiutfhl a beautiful day, the sun is shining and I love to do whatiseshining and I love \n",
      "\n",
      "\n",
      "iter :12000, loss:0.053459\n",
      "sy, the sun is shining and I love to do whatiteful day, the sun is shining and I love to do whatitIfwhatay, whatitiful day, the sun is shining and I love to do whatiu a beautiful day, the sun is shini\n",
      "\n",
      "\n",
      "iter :12500, loss:0.050303\n",
      "ing and I love to do whatin aabeautiful day, the sun is shining and I love to do whatis shining and I love to do whatisha beautiful day, the sun is shining and I love to do whatitifubeautiful day, the\n",
      "\n",
      "\n",
      "iter :13000, loss:0.047526\n",
      "iful day, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatinyfsbeautiful day, the sun is shining ant I love to do whatiu a beautiful day, the sun is\n",
      "\n",
      "\n",
      "iter :13500, loss:0.045057\n",
      "itifub a beautiful day, the sun is shining and I love to do whatiua is a beautiful day, the sun is shining and I love to do sho whatis a beautiful day, the sun is shining and I love to do whatiu a bea\n",
      "\n",
      "\n",
      "iter :14000, loss:0.042850\n",
      "ing and I love to do whatiu a beautiful day, the sun is shining and I love to do whatitoful day, the sun is shining and I love to do whatis shining and I love to do whatis a deautiful day, ihe sun is \n",
      "\n",
      "\n",
      "iter :14500, loss:0.040850\n",
      "iful day, the sun is shining and I love to do whatis shining and I love to do whatishshining and I love to do whatil day, the sun is shining and I love to do whatiuyf d I love to do whatiunf davetto d\n",
      "\n",
      "\n",
      "iter :15000, loss:0.039022\n",
      "aus is shining and I love to do whatiuishining and I love to do whatin s love to do whatitmful day, the sun is shining and I love to do whatitiful isg and I love to do whatiu a beautiful day, the sun \n",
      "\n",
      "\n",
      "iter :15500, loss:0.037346\n",
      "ing and I love to do whatil day, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatiloday, the sun is shining and I love to do whatitifudapdlbve to do\n",
      "\n",
      "\n",
      "iter :16000, loss:0.035806\n",
      "iful day, the sun is shining and I love to do whatit,fhl I love to do whatiu a beautiful day, the sun is shining and I love to do whatis shining and I love to do whatitiful day, the sun is shining and\n",
      "\n",
      "\n",
      "iter :16500, loss:0.034390\n",
      "ay, the sun is shining and I love to do whatiuning and I love to do whatiu a beautiful day, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatis a beautiful day, \n",
      "\n",
      "\n",
      "iter :17000, loss:0.033088\n",
      "ing and I love to do whatishaabeautiful day, the sun is shining and I love to do whatiu a beautiful day, the sun is shining and I love to do whatis shining and I love to do whatil day, the sun is shin\n",
      "\n",
      "\n",
      "iter :17500, loss:0.031881\n",
      "iful day, the sun is shining ahd I love to do whatiyfful day, the sun is shining and I love to do whatiunind I love to do whatiuning anis: it is a beautiful day, the sun is shining and I love to do wh\n",
      "\n",
      "\n",
      "iter :18000, loss:0.030762\n",
      "sy, the sun is shining and I love to do whatis shining and I love to do whatitdfubeautiful day, the sun is shining and I love to do whatiu.o dove to do whatitiful day, the sun is shining and I love to\n",
      "\n",
      "\n",
      "iter :18500, loss:0.029726\n",
      "ing and I love to do whatibefutiful day, the sun is shining and I love to do whatiu a beautiful day, the sun is shining and I love to do whatit,ful day, the sun is shining and I love to do whatis shin\n",
      "\n",
      "\n",
      "iter :19000, loss:0.028758\n",
      "iful day, the sun is shining and I love to do whatil aay, the sun is shining and I love to do whatitoful day, the sun is shining and I love to do whatis day, the sun is shining and I love to do whatiu\n",
      "\n",
      "\n",
      "iter :19500, loss:0.027853\n",
      "ito whatit,ful day, the sun is shining and I love to do whatiufful day, the sun is shining and I love to do whatinpasl day, the sun is shining and I love to do whatis shining and I love to do whatiudt\n",
      "\n",
      "\n",
      "iter :20000, loss:0.027009\n",
      "ing and I love to do whatit.ful day, the sun is shining and I love to do whatiutfutiful day, the sun is shining and I love to do whatiun is a beautiful day, the sun is shining and I love to do whatay,\n",
      "\n",
      "\n",
      "iter :20500, loss:0.026211\n",
      "iful day, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatiy,fud I love to do whatiuifubeautiful day, the sun is shining and I love to do whatiy,ful day, the su\n",
      "\n",
      "\n",
      "iter :21000, loss:0.025455\n",
      "ato whatio day, the sun is shining and I love to do whatit,ful day, the sun is shining and I love to do whatiudo dove to do whatitiful day, the sun is shining and I love to do whatiu shining and I lov\n",
      "\n",
      "\n",
      "iter :21500, loss:0.024741\n",
      "ing and I love to do whatiuning and I love to do whatul day, the sun is shining and I love to do whatiy, suning and I love to do whatitiful day, the sun is shining and I love to do whatil day, the sun\n",
      "\n",
      "\n",
      "iter :22000, loss:0.024061\n",
      "iful day, the sun is shining and I love to do whatishshining and I love to do whatil day, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatitofsu a b\n",
      "\n",
      "\n",
      "iter :22500, loss:0.023418\n",
      "ay, the sun is shining and I love to do whatiu shining and I love to do whatis shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatiHefutiful day, the sun is s\n",
      "\n",
      "\n",
      "iter :23000, loss:0.022814\n",
      "ing and I love to do whatiunind I love to do whatis shining and I love to do whatil day, the sun is shining and I love to do whatwhatis aabeautiful day, the sun is shining and I love to do whatiy,ful \n",
      "\n",
      "\n",
      "iter :23500, loss:0.022239\n",
      "iful day, the sun is shining and I love to do whatiun is a beautiful day, the sun is shining and I love to do whatibeautiful day, the sun is shining and I love to do whatis a beautiful day, the sun is\n",
      "\n",
      "\n",
      "iter :24000, loss:0.021694\n",
      "ay, the sun is shining and I love to do whatiusa beautiful day, the sun is shining and I love to do whatay, the sun is shining and I love to do whatwhatiuntay, the sun is shining and I love to do what\n",
      "\n",
      "\n",
      "iter :24500, loss:0.021180\n",
      "ing and I love to do whatibeautiful day, the sun is shining and I love to do whatiupiib and I love to do whatiuna d a love to do whatin fay, the sun is shining and I love to do whatis a beautiful day,\n",
      "\n",
      "\n",
      "iter :25000, loss:0.020688\n",
      "iful day, the sun is shining and I love to do whatiun ib a beautiful day, the sun is shining and I love to do whatiutful day, the sun is shining and I love to do whatis a beautiful day, the sun is shi\n",
      "\n",
      "\n",
      "iter :25500, loss:0.020219\n",
      "ay, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatit,ful day, the sun is shining and I love to do whatibeautiful day, the sun is shining and I love to do what\n",
      "\n",
      "\n",
      "iter :26000, loss:0.019774\n",
      "ing and I love to do whatitifubeautiful day, the sun is shining and I love to do whatiuns beautiful day, the sun is shining and I love to do whatineasl a beautiful day, the sun is shining and I love t\n",
      "\n",
      "\n",
      "iter :26500, loss:0.019347\n",
      "iful day, the sun is shining and I love to do whatiuning a beautiful day, the sun is shining and I love to do whatiunisg and I love to do whatis t beautiful day, the sun is shining and I love to do wh\n",
      "\n",
      "\n",
      "iter :27000, loss:0.018938\n",
      "itiful day, the sun is shining and I love to do whatwhatit,futiful day, the sun is shining and I love to do whatay, the sun is shining and I love to do whatiy,ful day, the sun is shining and I love to\n",
      "\n",
      "\n",
      "iter :27500, loss:0.018548\n",
      "ing and I love to do whatiy, th I love to do whatiu, is a beautiful day, the sun is shining and I love to do whatiun is a beautiful day, the sun is shining and I love to do whatitofayaisg and I love t\n",
      "\n",
      "\n",
      "iter :28000, loss:0.018171\n",
      "iful day, the sun is shining and I love to do whatiu a beautiful day, the sun is shining and I love to do whatiy, whatilbfweatitiful day, the sun is shining and I love to do whatih tay, the sun is shi\n",
      "\n",
      "\n",
      "iter :28500, loss:0.017810\n",
      "ay, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatiutful day, the sun is shining and I love to do whatibefutiful day, the sun is shining and I lov\n",
      "\n",
      "\n",
      "iter :29000, loss:0.017465\n",
      "ing and I love to do whatiuta t a beautiful day, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatibe\n",
      "\n",
      "\n",
      "iter :29500, loss:0.017130\n",
      "iful day, the sun is shining and I love to do whatiunisg and I love to do whatil day, the sun is shining and I love to do whatishshining and I love to do whatitdfubeautiful day, the sun is shining atd\n",
      "\n",
      "\n",
      "iter :30000, loss:0.016807\n",
      "ay, the sun is shining and I love to do whatiuni b autiful day, the sun is shining aud I love to do whatiy,ful day, the tun is shining and I love to do whatis a beautiful day, the sun is shining and I\n",
      "\n",
      "\n",
      "iter :30500, loss:0.016498\n",
      "ing and I love to do whatiundido whatilove to do whatis a beautiful day, the sun is shining and I love to do whatiunisg and I love to do whatitofay aabeautiful day, the sun is shining and I love to do\n",
      "\n",
      "\n",
      "iter :31000, loss:0.016198\n",
      "iful day, the sun is shining and I love to do whatiuns beautiful day, the sun is shining and I love to do whatiptfadove to do whatitHful day, the sun is shining and I love to do whatiuna beautiful day\n",
      "\n",
      "\n",
      "iter :31500, loss:0.015907\n",
      "wyatil day, the sun is shining and I love to do whatibeautiful day, the sun is shining and I love to do whatinovatt whatis shining and I love to do whatinofabeautiful day, the sun is shining and I lov\n",
      "\n",
      "\n",
      "iter :32000, loss:0.015627\n",
      "ing and I love to do whatitgful day, the sun is shining and I love to do whatiuo is a beautiful day, the sun is shining and I love to do whatil fay, the sun is shining and I love to doIwhatiuntis a be\n",
      "\n",
      "\n",
      "iter :32500, loss:0.015353\n",
      "iful day, the sun is shining and I love to do whatig aay, the sun is shining and I love to do whatingfsbeautiful day, the sun is shining and I love to do whatihe sun is shining and I love to do whatiy\n",
      "\n",
      "\n",
      "iter :33000, loss:0.015086\n",
      "iy, the sun is shining and I love to do whatwlatwhatitifub autiful day, the sun is shining and I love to do whatiupHibeautiful day, the sun is shining and I love to do whatit:ful day, the sun is shini\n",
      "\n",
      "\n",
      "iter :33500, loss:0.014827\n",
      "ing and I love to do whatay, the sun is shining and I love to do whatiuta s a beautiful day, the sun is shining and I love to do whatis shining and I love to do whatiunasbeautiful day, the sun is shin\n",
      "\n",
      "\n",
      "iter :34000, loss:0.014573\n",
      "iful day, the sun is shining and I love to do whatiuda beautiful day, the sun is shining and I love to do whatiutful day, the sun is shining and I love to do whatitdful day, the sun is shining and I l\n",
      "\n",
      "\n",
      "iter :34500, loss:0.014327\n",
      "ay, the sun is shining and I love to do whatitofuy aay, the sun is shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatiud is a beautiful day, the sun is shini\n",
      "\n",
      "\n",
      "iter :35000, loss:0.014089\n",
      "ing and I love to do whatis a beautiful day, the sun is shining and I love to do whatitishining and I love to do whatiuntib anbeautiful day, the sun is shining and I love to do whatiunais a beautiful \n",
      "\n",
      "\n",
      "iter :35500, loss:0.013858\n",
      "iful day, the sun is shining and I love to do whatis a beautiful day, the sun is shining and I love to do whatitmful day, the sun is shining and I love to do whatiunday, the sun is shining and I love \n",
      "\n",
      "\n",
      "iter :36000, loss:0.013634\n",
      "ilove to do whatiuto dove to do whatit:ful day, the sun is shining and I love to do whatiloday, the sun is shining and I love to do whatiu ing and I love to do whatitwful I love to do whatilovehe sun \n",
      "\n",
      "\n",
      "iter :36500, loss:0.013419\n",
      "ing and I love to do whatiund do whatiuvs beautiful day, the sun is shining and I love to do whatitdful day, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatiun\n",
      "\n",
      "\n",
      "iter :37000, loss:0.013208\n",
      "iful day, the sun is shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatiut ss a beautiful day, the sun is shining and I love to do whatiut su a d I love to d\n",
      "\n",
      "\n",
      "iter :37500, loss:0.013003\n",
      "sto whatitiful day, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatiu a beautiful day, the sun is shining and I love to do whatiy,fsuning and I love to do what\n",
      "\n",
      "\n",
      "iter :38000, loss:0.012804\n",
      " is a beautiful day, the sun is shining and I love to do whatag aado whatiu inbeautiful day, the sun is shining and I love to do whatisofabeautiful day, the sun is shining and I love to do whatiuns be\n",
      "\n",
      "\n",
      "iter :38500, loss:0.012608\n",
      "iful day, the sun is shining and I love to do whatitiful day, the sun is shining and I love to do whatil day, the sun is shining and I love to do whatibeaag a beautiful day, the sun is shining and I l\n",
      "\n",
      "\n",
      "iter :39000, loss:0.012416\n",
      "isoshining and I love to do whatiudful day, the sun is shining and I love to do whatiydful day, the sun is shining and I love to do whatibeautiful day, the sun is shining and I love to do whatisda bea\n",
      "\n",
      "\n",
      "iter :39500, loss:0.012232\n",
      "ing and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatil auy, the sun is shining and I love to do whatiuna d I love to do whatiuns beautiful day, the sun is shining \n",
      "\n",
      "\n",
      "iter :40000, loss:0.012053\n",
      "iful day, the sun is shining and I love to do whatido whatiupful day, the sun is shining and I love to do whatihetsun is shining and I love to do whatil day, the sun is shining and I love to do whatiu\n",
      "\n",
      "\n",
      "iter :40500, loss:0.011879\n",
      "sy, the sun is shining and I love to do whatiuaful day, the sun is shining and I love to do whatiunv te the sun is shining and I love to do whatiun is a beautiful day, the sun is shining and I love to\n",
      "\n",
      "\n",
      "iter :41000, loss:0.011712\n",
      "ing and I love to do whatitofal day, the sun is shining and I love to do whatitofup day, the sun is shining and I love to do whatitdful day, the sun is shining and I love to do whatis a beautiful day,\n",
      "\n",
      "\n",
      "iter :41500, loss:0.011547\n",
      "iful day, the sun is shining and I love to do whatiunsui a ining and I love to do whatiuniid I love to do whatiyoful day, the sun is shining and I love to do whatiuI is a beautiful day, the sun is shi\n",
      "\n",
      "\n",
      "iter :42000, loss:0.011387\n",
      "ito whatitefutiful day, the sun is shining and I love to do whatitifhl a beautiful day, the sun is shining and I love to do whatitiful day, the sun is shining and I love to do whatis shining and I lov\n",
      "\n",
      "\n",
      "iter :42500, loss:0.011232\n",
      "ing and I love to do whatiunaib aubeautiful day, the sun is shining and I love to do whatitpful day, the sun is shining and I love to do whatittful day, the sun is shining and I love to do whatit,ful \n",
      "\n",
      "\n",
      "iter :43000, loss:0.011080\n",
      "iful day, the sun is shining and I love to do whatiy,ful day, the sun is shining and I love to do whatitiful day, the sun is shining and I love to do whatitiful day, the sun is shining and I love to d\n",
      "\n",
      "\n",
      "iter :43500, loss:0.010932\n",
      "ay, the sun is shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatitiful day, the sun is shining and I love to do whatitgfay a beautiful day, the sun is shini\n",
      "\n",
      "\n",
      "iter :44000, loss:0.010788\n",
      "ing and I love to do whatis s beautiful day, the sun is shining and I love to do whatibe utiful day, the sun is shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do\n",
      "\n",
      "\n",
      "iter :44500, loss:0.010646\n",
      "iful day, the sun is shining and I love to do whatiu a beautiful day, the sun is shining and I love to do whatisafhining and I love to do whatiut is a beautiful day, the sun is shining and I love to d\n",
      "\n",
      "\n",
      "iter :45000, loss:0.010507\n",
      "ay, the sun is shining and I love to do whati,tfsbeautiful day, the sun is shining and I love to do whatwhatiutfis a beautiful day, the sun is shining and I love to do whatilove to do whatihatwyatiuov\n",
      "\n",
      "\n",
      "iter :45500, loss:0.010373\n",
      "ing and I love to do whatindfsy, the sun is shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatitiful day, the sun is shining and I love to do whatiuto do wha\n",
      "\n",
      "\n",
      "iter :46000, loss:0.010240\n",
      "iful day, the sun is shining and I love to do whatiunisg and I love to do whatiy,ful day, the sun is shining and I love to do whatiut is a beautiful day, the sun is shining and I love to do whatiteful\n",
      "\n",
      "\n",
      "iter :46500, loss:0.010111\n",
      "Epoch 1/25 completed with loss: 0.009999395064993774\n",
      "Epoch 2/25 completed with loss: 0.009999395064993774\n",
      "Epoch 3/25 completed with loss: 0.009999395064993774\n",
      "Epoch 4/25 completed with loss: 0.009999395064993774\n",
      "Epoch 5/25 completed with loss: 0.009999395064993774\n",
      "Epoch 6/25 completed with loss: 0.009999395064993774\n",
      "Epoch 7/25 completed with loss: 0.009999395064993774\n",
      "Epoch 8/25 completed with loss: 0.009999395064993774\n",
      "Epoch 9/25 completed with loss: 0.009999395064993774\n",
      "Epoch 10/25 completed with loss: 0.009999395064993774\n",
      "Epoch 11/25 completed with loss: 0.009999395064993774\n",
      "Epoch 12/25 completed with loss: 0.009999395064993774\n",
      "Epoch 13/25 completed with loss: 0.009999395064993774\n",
      "Epoch 14/25 completed with loss: 0.009999395064993774\n",
      "Epoch 15/25 completed with loss: 0.009999395064993774\n",
      "Epoch 16/25 completed with loss: 0.009999395064993774\n",
      "Epoch 17/25 completed with loss: 0.009999395064993774\n",
      "Epoch 18/25 completed with loss: 0.009999395064993774\n",
      "Epoch 19/25 completed with loss: 0.009999395064993774\n",
      "Epoch 20/25 completed with loss: 0.009999395064993774\n",
      "Epoch 21/25 completed with loss: 0.009999395064993774\n",
      "Epoch 22/25 completed with loss: 0.009999395064993774\n",
      "Epoch 23/25 completed with loss: 0.009999395064993774\n",
      "Epoch 24/25 completed with loss: 0.009999395064993774\n",
      "Epoch 25/25 completed with loss: 0.009999395064993774\n",
      "hello dsun sun is a beautiful day, the sun is shining a\n"
     ]
    }
   ],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "    \n",
    "    \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "        xs, hs, os, ycap = {}, {}, {}, {}\n",
    "        hs[-1] = np.copy(hprev)\n",
    "        for t in range(len(inputs)):\n",
    "            xs[t] = zero_init(self.vocab_size,1)\n",
    "            xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "            hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "            os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "            ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "        return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "        # backward pass: compute gradients going backwards\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "        dhnext = np.zeros_like(hs[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            dy = np.copy(ps[t])\n",
    "            #through softmax\n",
    "            dy[targets[t]] -= 1 # backprop into y\n",
    "            #calculate dV, dc\n",
    "            dV += np.dot(dy, hs[t].T)\n",
    "            dc += dc\n",
    "            #dh includes gradient from two sides, next cell and current output\n",
    "            dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "            # backprop through tanh non-linearity \n",
    "            dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "            db += dhrec\n",
    "            #calculate dU and dW\n",
    "            dU += np.dot(dhrec, xs[t].T)\n",
    "            dW += np.dot(dhrec, hs[t-1].T)\n",
    "            #pass the gradient from next cell to the next iteration.\n",
    "            dhnext = np.dot(self.W.T, dhrec)\n",
    "        # clip to mitigate exploding gradients\n",
    "        for dparam in [dU, dW, dV, db, dc]:\n",
    "            np.clip(dparam, -5, 5, out=dparam) \n",
    "        return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "        \"\"\"loss for a sequence\"\"\"\n",
    "        # calculate cross-entrpy loss\n",
    "        return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "        \"\"\"\n",
    "        sample a sequence of integers from the model\n",
    "        h is memory state, seed_ix is seed letter from the first time step\n",
    "        \"\"\"\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        x[seed_ix] = 1\n",
    "        ixes = []\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        return ixes\n",
    "\n",
    "\n",
    "    def train(self, data_reader, num_epochs=10):\n",
    "        iter_num = 0\n",
    "        threshold = 0.01\n",
    "        smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "        for epoch in range(num_epochs):\n",
    "            while (smooth_loss > threshold):\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num % 500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print(''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print(\"\\n\\niter :%d, loss:%f\" % (iter_num, smooth_loss))\n",
    "                iter_num += 1\n",
    "            print(f\"Epoch {epoch + 1}/{num_epochs} completed with loss: {smooth_loss}\")\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt\n",
    "\n",
    "# To read the training data and make a vocabulary and dictionary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()\n",
    "\n",
    "# Example usage\n",
    "# Create a DataReader instance\n",
    "data_reader = DataReader('/Users/jasper/Desktop/RNN_from_scratch/input.txt', seq_length=25)\n",
    "\n",
    "# Create an RNN instance\n",
    "hidden_size = 300\n",
    "seq_length = 25\n",
    "learning_rate = 0.05\n",
    "rnn = RNN(hidden_size, data_reader.vocab_size, seq_length, learning_rate)\n",
    "\n",
    "# Train the RNN for a specified number of epochs\n",
    "num_epochs = 25\n",
    "rnn.train(data_reader, num_epochs=num_epochs)\n",
    "\n",
    "# Predict the next characters starting with a given word\n",
    "start_word = \"hello\"\n",
    "num_chars_to_predict = 50\n",
    "predicted_text = rnn.predict(data_reader, start_word, num_chars_to_predict)\n",
    "\n",
    "print(predicted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

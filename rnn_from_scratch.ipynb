{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPLEMENTING RNN FROM SCRATCH IN PYTHON"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implementation¶\n",
    "We will implement a full Recurrent Neural Network from scratch using Python. We will try to build a text generation model using an RNN. We train our model to predict the probability of a character given the preceding characters. It’s a generative model. Given an existing sequence of characters we sample a next character from the predicted probabilities, and repeat the process until we have a full sentence. This implementation is from Andrej Karparthy great post building a character level RNN. Here we will discuss the implementation details step by step.\n",
    "\n",
    "General steps to follow:\n",
    "\n",
    "Initialize weight matrices U, V, W from random distribution and bias b, c with zeros\n",
    "Forward propagation to compute predictions\n",
    "Compute the loss\n",
    "Back-propagation to compute gradients\n",
    "Update weights based on gradients\n",
    "Repeat steps 2–5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Initialize:\n",
    "\n",
    "To start with the implementation of the basic RNN cell, we first define the dimensions of the various parameters U,V,W,b,c.\n",
    "\n",
    "Dimensions: Let’s assume we pick a vocabulary size vocab_size= 8000 and a hidden layer size hidden_size=100. Then we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self,hidden_size,vocab_size,seq_length,learning_rate):\n",
    "        # hyper params\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # model params\n",
    "        \n",
    "        self.U= np.random.uniform(-np.sqrt(1./vocab_size),np.sqrt(1./vocab_size),(hidden_size,vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size),np.sqrt(1./hidden_size),(vocab_size,hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size),np.sqrt(1./hidden_size),(hidden_size,hidden_size))\n",
    "        self.b = np.zeros((hidden_size,1)) # bias for hidden layer.\n",
    "        self.c = np.zeros((vocab_size,1)) # bias for output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Proper initialization of weights seems to have an impact on training results there has been lot of research in this area. It turns out that the best initialization depends on the activation function (tanh in our case) and one recommended approach is to initialize the weights randomly in the interval from[ -1/sqrt(n), 1/sqrt(n)]where n is the number of incoming connections from the previous layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Forward pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Straightforward as per our equations for each timestamp t, we calculate hidden state hs[t] and output os[t] applying softmax to get the probability for the next character."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self,inputs,hprev):\n",
    "    xs,hs,os,ycap = {},{},{},{}\n",
    "    hs[-1] = np.copy(hprev)\n",
    "    for t in range(len(inputs)):\n",
    "        xs[t] = zero_init(self.vocab_size,1)\n",
    "        xs[t][inputs[t]] = 1 # one hot encoding\n",
    "        hs[t] = np.tanh(np.dot(self.U,xs[t])+np.dot(self.W,hs[t-1])+ self.b) # hidden state\n",
    "        os[t] = np.dot(self.V,hs[t]) + self.c\n",
    "        ycap[t] = self.softmax(os[t])\n",
    "    return xs,hs,ycap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps/ np.sum(exps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.09003057, 0.24472847, 0.66524096])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/9c/qp21xvjj4tq4tgn3cw0gltn40000gn/T/ipykernel_22712/3264919273.py:2: RuntimeWarning: overflow encountered in exp\n",
      "  exps = np.exp(x)\n",
      "/var/folders/9c/qp21xvjj4tq4tgn3cw0gltn40000gn/T/ipykernel_22712/3264919273.py:3: RuntimeWarning: invalid value encountered in divide\n",
      "  return exps/ np.sum(exps)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([nan, nan, nan])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "softmax([1000,2000,3000])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We adjust the softmax function to handle larger numbers like this:\n",
    "\n",
    "def softmax(self,x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p / np.sum(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "def softmax(x):\n",
    "    p = np.exp(x - np.max(x))\n",
    "    return p / np.sum(p)\n",
    "\n",
    "# Example usage\n",
    "print(softmax([1, 2, 3]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.09003057 0.24472847 0.66524096]\n"
     ]
    }
   ],
   "source": [
    "\"\"\"import numpy as np\n",
    "\n",
    "class MyClass:\n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x - np.max(x))\n",
    "        return p / np.sum(p)\n",
    "\n",
    "# Create an instance of the class\n",
    "my_instance = MyClass()\n",
    "\n",
    "# Example usage\n",
    "print(my_instance.softmax([1, 2, 3]))\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss(self,ps,targets):\n",
    "    '''loss for a sequence'''\n",
    "    # cross entropy loss\n",
    "    return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Backward pass¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step 5: Update weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Repeat steps 2–5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order for our model to learn from the data and generate text, we need to train it for sometime and check loss after each iteration. If the loss is reducing over a period of time that means our model is learning what is expected of it.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate Text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train for some time and if all goes well, we should have our model ready to predict some text. Let us see how it works for us.\n",
    "\n",
    "We will implement a predict method to predict few words like below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def zero_init(rows, cols):\n",
    "    return np.zeros((rows, cols))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN:\n",
    "    def __init__(self, hidden_size, vocab_size, seq_length, learning_rate):\n",
    "        # hyper parameters\n",
    "        self.hidden_size = hidden_size\n",
    "        self.vocab_size = vocab_size\n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        # model parameters\n",
    "        self.U = np.random.uniform(-np.sqrt(1./vocab_size), np.sqrt(1./vocab_size), (hidden_size, vocab_size))\n",
    "        self.V = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (vocab_size, hidden_size))\n",
    "        self.W = np.random.uniform(-np.sqrt(1./hidden_size), np.sqrt(1./hidden_size), (hidden_size, hidden_size))\n",
    "        self.b = np.zeros((hidden_size, 1)) # bias for hidden layer\n",
    "        self.c = np.zeros((vocab_size, 1)) # bias for output\n",
    "        \n",
    "        # memory vars for adagrad, \n",
    "        #ignore if you implement another approach\n",
    "        self.mU = np.zeros_like(self.U)\n",
    "        self.mW = np.zeros_like(self.W)\n",
    "        self.mV = np.zeros_like(self.V)\n",
    "        self.mb = np.zeros_like(self.b)\n",
    "        self.mc = np.zeros_like(self.c)\n",
    "    \n",
    "    def softmax(self, x):\n",
    "        p = np.exp(x- np.max(x))\n",
    "        return p / np.sum(p)\n",
    "        \n",
    "    def forward(self, inputs, hprev):\n",
    "            xs, hs, os, ycap = {}, {}, {}, {}\n",
    "            hs[-1] = np.copy(hprev)\n",
    "            for t in range(len(inputs)):\n",
    "                xs[t] = zero_init(self.vocab_size,1)\n",
    "                xs[t][inputs[t]] = 1 # one hot encoding , 1-of-k\n",
    "                hs[t] = np.tanh(np.dot(self.U,xs[t]) + np.dot(self.W,hs[t-1]) + self.b) # hidden state\n",
    "                os[t] = np.dot(self.V,hs[t]) + self.c # unnormalised log probs for next char\n",
    "                ycap[t] = self.softmax(os[t]) # probs for next char\n",
    "            return xs, hs, ycap\n",
    "        \n",
    "    def backward(self, xs, hs, ps, targets):\n",
    "            # backward pass: compute gradients going backwards\n",
    "            dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "            db, dc = np.zeros_like(self.b), np.zeros_like(self.c)\n",
    "            dhnext = np.zeros_like(hs[0])\n",
    "            for t in reversed(range(self.seq_length)):\n",
    "                dy = np.copy(ps[t])\n",
    "                #through softmax\n",
    "                dy[targets[t]] -= 1 # backprop into y\n",
    "                #calculate dV, dc\n",
    "                dV += np.dot(dy, hs[t].T)\n",
    "                dc += dc\n",
    "                #dh includes gradient from two sides, next cell and current output\n",
    "                dh = np.dot(self.V.T, dy) + dhnext # backprop into h\n",
    "                # backprop through tanh non-linearity \n",
    "                dhrec = (1 - hs[t] * hs[t]) * dh  #dhrec is the term used in many equations\n",
    "                db += dhrec\n",
    "                #calculate dU and dW\n",
    "                dU += np.dot(dhrec, xs[t].T)\n",
    "                dW += np.dot(dhrec, hs[t-1].T)\n",
    "                #pass the gradient from next cell to the next iteration.\n",
    "                dhnext = np.dot(self.W.T, dhrec)\n",
    "            # clip to mitigate exploding gradients\n",
    "            for dparam in [dU, dW, dV, db, dc]:\n",
    "                np.clip(dparam, -5, 5, out=dparam) \n",
    "            return dU, dW, dV, db, dc\n",
    "    \n",
    "    def loss(self, ps, targets):\n",
    "            \"\"\"loss for a sequence\"\"\"\n",
    "            # calculate cross-entrpy loss\n",
    "            return sum(-np.log(ps[t][targets[t],0]) for t in range(self.seq_length))\n",
    "        \n",
    "    \n",
    "    def update_model(self, dU, dW, dV, db, dc):\n",
    "        # parameter update with adagrad\n",
    "        for param, dparam, mem in zip([self.U, self.W, self.V, self.b, self.c],\n",
    "                                  [dU, dW, dV, db, dc],\n",
    "                                  [self.mU, self.mW, self.mV, self.mb, self.mc]):\n",
    "            mem += dparam*dparam\n",
    "            param += -self.learning_rate*dparam/np.sqrt(mem+1e-8) # adagrad update\n",
    "                \n",
    "                \n",
    "    def sample(self, h, seed_ix, n):\n",
    "            \"\"\"\n",
    "            sample a sequence of integers from the model\n",
    "            h is memory state, seed_ix is seed letter from the first time step\n",
    "            \"\"\"\n",
    "            x = zero_init(self.vocab_size, 1)\n",
    "            x[seed_ix] = 1\n",
    "            ixes = []\n",
    "            for t in range(n):\n",
    "                h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "                y = np.dot(self.V, h) + self.c\n",
    "                p = np.exp(y)/np.sum(np.exp(y))\n",
    "                ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "                x = zero_init(self.vocab_size,1)\n",
    "                x[ix] = 1\n",
    "                ixes.append(ix)\n",
    "            return ixes\n",
    "\n",
    "    def train(self, data_reader):\n",
    "            iter_num = 0\n",
    "            threshold = 0.01\n",
    "            smooth_loss = -np.log(1.0/data_reader.vocab_size)*self.seq_length\n",
    "            while (smooth_loss > threshold):\n",
    "                if data_reader.just_started():\n",
    "                    hprev = np.zeros((self.hidden_size,1))\n",
    "                inputs, targets = data_reader.next_batch()\n",
    "                xs, hs, ps = self.forward(inputs, hprev)\n",
    "                dU, dW, dV, db, dc = self.backward(xs, hs, ps, targets)\n",
    "                loss = self.loss(ps, targets)\n",
    "                self.update_model(dU, dW, dV, db, dc)\n",
    "                smooth_loss = smooth_loss*0.999 + loss*0.001\n",
    "                hprev = hs[self.seq_length-1]\n",
    "                if not iter_num%500:\n",
    "                    sample_ix = self.sample(hprev, inputs[0], 200)\n",
    "                    print( ''.join(data_reader.ix_to_char[ix] for ix in sample_ix))\n",
    "                    print( \"\\n\\niter :%d, loss:%f\"%(iter_num, smooth_loss))\n",
    "                iter_num += 1\n",
    "\n",
    "    def predict(self, data_reader, start, n):\n",
    "\n",
    "        #initialize input vector\n",
    "        x = zero_init(self.vocab_size, 1)\n",
    "        chars = [ch for ch in start]\n",
    "        ixes = []\n",
    "        for i in range(len(chars)):\n",
    "            ix = data_reader.char_to_ix[chars[i]]\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "\n",
    "        h = np.zeros((self.hidden_size,1))\n",
    "        # predict next n chars\n",
    "        for t in range(n):\n",
    "            h = np.tanh(np.dot(self.U, x) + np.dot(self.W, h) + self.b)\n",
    "            y = np.dot(self.V, h) + self.c\n",
    "            p = np.exp(y)/np.sum(np.exp(y))\n",
    "            ix = np.random.choice(range(self.vocab_size), p = p.ravel())\n",
    "            x = zero_init(self.vocab_size,1)\n",
    "            x[ix] = 1\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(data_reader.ix_to_char[i] for i in ixes)\n",
    "        return txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# To read the training data and make a vocabulary and dictiornary to index the chars\n",
    "class DataReader:\n",
    "    def __init__(self, path, seq_length):\n",
    "        #uncomment below , if you dont want to use any file for text reading and comment next 2 lines\n",
    "        #self.data = \"some really long text to test this. maybe not perfect but should get you going.\"\n",
    "        self.fp = open(path, \"r\")\n",
    "        self.data = self.fp.read()\n",
    "        #find unique chars\n",
    "        chars = list(set(self.data))\n",
    "        #create dictionary mapping for each char\n",
    "        self.char_to_ix = {ch:i for (i,ch) in enumerate(chars)}\n",
    "        self.ix_to_char = {i:ch for (i,ch) in enumerate(chars)}\n",
    "        #total data\n",
    "        self.data_size = len(self.data)\n",
    "        #num of unique chars\n",
    "        self.vocab_size = len(chars)\n",
    "        self.pointer = 0\n",
    "        self.seq_length = seq_length\n",
    "\n",
    "    def next_batch(self):\n",
    "        input_start = self.pointer\n",
    "        input_end = self.pointer + self.seq_length\n",
    "        inputs = [self.char_to_ix[ch] for ch in self.data[input_start:input_end]]\n",
    "        targets = [self.char_to_ix[ch] for ch in self.data[input_start+1:input_end+1]]\n",
    "        self.pointer += self.seq_length\n",
    "        if self.pointer + self.seq_length + 1 >= self.data_size:\n",
    "            # reset pointer\n",
    "            self.pointer = 0\n",
    "        return inputs, targets\n",
    "\n",
    "    def just_started(self):\n",
    "        return self.pointer == 0\n",
    "\n",
    "    def close(self):\n",
    "        self.fp.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "geit ilmfsefsbeei:edsnessbhss.umohv:aitmsnsw.Ive:aatpeis aspogaay.fhhgsss,Iivmsfslassssfylhdg.,Hseasb:stessisiae:msHpssmlwt,I,aaHbeysoa asbop:yunisiissvhlsse psgaagiseosmepssisvusbpitIwssisba,.w.ssHus\n",
      "\n",
      "\n",
      "iter :0, loss:80.472036\n",
      " day, the sun is s beaulip l day, the sun iu shinin ss a beautiful day, the sun is shining and I love tl do whattpas I beautiful day, the sun isishiningnand and do whattin as shiningnan is shining and\n",
      "\n",
      "\n",
      "iter :500, loss:58.984736\n",
      "ing I love to do whathin is shining and I love to do whatt aya i  do whatisul day, the sun is shining ao bha t is shining and I love to do whatt whattin: s andnI lovedto do whato whatt e ss  if shinin\n",
      "\n",
      "\n",
      "iter :1000, loss:35.917290\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatting s d le utifulivn is shining and I love to do whato bpis a beautiful day, the sun is shining and\n",
      "\n",
      "\n",
      "iter :1500, loss:21.843763\n",
      "o whatiin  s dove to do whatting s dove to do whattin is shining and I love to do whattis is shining and I love to do whatiful day, the sun is shining and I love to do whatove sun is shining and I lov\n",
      "\n",
      "\n",
      "iter :2000, loss:13.286743\n",
      "ing and I love to do whato whatiful day, the sun is shining and I love to do whatifnl day, the sun is shining and I love to do whatt esss ii is a beautiful day, the sun is shining and I love to do wha\n",
      "\n",
      "\n",
      "iter :2500, loss:8.086961\n",
      "iful day, the sun is shining and I love to do whato whattind I love to do whatt essshinb and I love to do whato e to any,d to whato wua is a beautiful day, the sun is shining and I love to do whattpnd\n",
      "\n",
      "\n",
      "iter :3000, loss:4.927512\n",
      "o whatiful day, the sun is shining and I love to do what, whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whattin is shining and I love to do whatif\n",
      "\n",
      "\n",
      "iter :3500, loss:3.007491\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whatt is shining and I love to do whato whatting and I love to do whatiful day, the sun is shining and I love to do whatiful doy,\n",
      "\n",
      "\n",
      "iter :4000, loss:1.840253\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato is shining and I love to do whatiful day, the sun is shining and I love to do whatt is shining and\n",
      "\n",
      "\n",
      "iter :4500, loss:1.130268\n",
      "o whatiful day, thelsun is shining and I love to do whathind I love to do whatiful day, the sun is shining and I love to do whato whato whatiful day, the sun is shining and I love to do whatidisus in \n",
      "\n",
      "\n",
      "iter :5000, loss:0.698053\n",
      " is ahbnasu duy dnd Iay,ve to d I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato n as shining and I love to do whatiful day, the s\n",
      "\n",
      "\n",
      "iter :5500, loss:0.434602\n",
      "iful day, the sun is shining and I love to do whatifil day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatt is shining and I love to do whato ay, the sun is\n",
      "\n",
      "\n",
      "iter :6000, loss:0.273754\n",
      "o whatt es s beautiful day, the sun is shining and I love to do whatt is shining and I love to do whatiful day, the sun is shining and I love to do whatt is shining and I love to do whathfnd Ial ve tt\n",
      "\n",
      "\n",
      "iter :6500, loss:0.175312\n",
      "ing and I love to do whato e ss a beautiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatt whatiful \n",
      "\n",
      "\n",
      "iter :7000, loss:0.114847\n",
      "iful day, the sun is shining and I love to do whato isushining and I love to do whatifbl day, the sun is shining and I love to do whato whathing and I love to do whato whatiful day, the sun is shining\n",
      "\n",
      "\n",
      "iter :7500, loss:0.077533\n",
      "o whatiful day, the sun is shining and I love to do whato whatt as shining and I love to do whatifuliday, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato wh\n",
      "\n",
      "\n",
      "iter :8000, loss:0.054347\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatting and I love to do wha\n",
      "\n",
      "\n",
      "iter :8500, loss:0.039792\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do what\n",
      "\n",
      "\n",
      "iter :9000, loss:0.030535\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato whatiful day, the sun is shining and I love to d\n",
      "\n",
      "\n",
      "iter :9500, loss:0.024540\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whato whatiful day, the sun is shining and I love to do whato whato is shining and I love to do whattfuy, the sun is shining and \n",
      "\n",
      "\n",
      "iter :10000, loss:0.020558\n",
      "iful day, the sun is shining and Iolove to do whatt as d be ut ds whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whattfuy, th ds: it is shining and\n",
      "\n",
      "\n",
      "iter :10500, loss:0.017837\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatn nI I bevuttfuliful day, the sun is shining and I love to do whatiful day, the sun is shining and I\n",
      "\n",
      "\n",
      "iter :11000, loss:0.015913\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whatoinsis shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love\n",
      "\n",
      "\n",
      "iter :11500, loss:0.014492\n",
      "iful day, the sun is shining and I love to do whattiu is a beautiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whato whatt is shining and I love to do \n",
      "\n",
      "\n",
      "iter :12000, loss:0.013406\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do what\n",
      "\n",
      "\n",
      "iter :12500, loss:0.012543\n",
      "ing and I love to do whatiful day, the sun is shining and I love to do whathin is shining and I love to do whatn ng s d beautnfulivn is shining and I love to do whattind I love to do whathfnl I, isa s\n",
      "\n",
      "\n",
      "iter :13000, loss:0.011828\n",
      "iful day, the sun is shining and I love to do whathing a l ve to do whatiful day,pthe sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining\n",
      "\n",
      "\n",
      "iter :13500, loss:0.011222\n",
      "iful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whathing and I love to do whattfulw a: it is a ining a\n",
      "\n",
      "\n",
      "iter :14000, loss:0.010697\n",
      "ing and I love to do whathin is shining and I love to do what, whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and I love to do whatiful day, the sun is shining and \n",
      "\n",
      "\n",
      "iter :14500, loss:0.010229\n"
     ]
    }
   ],
   "source": [
    "seq_length = 25\n",
    "#read text from the \"input.txt\" file\n",
    "data_reader = DataReader(\"/Users/jasper/Desktop/RNN_from_scratch/input.txt\", seq_length)\n",
    "rnn = RNN(hidden_size=100, vocab_size=data_reader.vocab_size,seq_length=seq_length,learning_rate=1e-1)\n",
    "rnn.train(data_reader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
